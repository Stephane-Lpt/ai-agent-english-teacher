services:
  web:
    image: node:alpine
    container_name: web_service_front
    volumes:
      - ./front:/usr/src/app
    working_dir: /usr/src/app
    ports:
      - "8080:8080"
    command: ["npx", "http-server", "-p", "8080"]
    restart: always
    depends_on:
      fastapi_backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f 'http://localhost:8080/' || exit 1"]
      interval: 30s
      retries: 20

  fastapi_backend:
    build:
      context: ./back  # Path to the FastAPI project
    container_name: fastapi_backend
    ports:
      - "8000:8000"  # Map FastAPI default port
    volumes:
      - ./back:/app  # Sync local code with container
    environment:
      - PYTHONUNBUFFERED=1  # Ensures logs are shown immediately
    stdin_open: true  # Keep container interactive
    tty: true  # Enable terminal interaction
    command: ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    depends_on:
      postgres:
        condition: service_healthy
      asr_service:
        condition: service_healthy
      tts_service:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f 'http://localhost:8000/docs' || exit 1"]
      interval: 30s
      retries: 20

  asr_service:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    container_name: asr_service
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=openai_whisper
    healthcheck:
      test: ["CMD-SHELL", "curl -f 'http://localhost:9000/docs'  || exit 1"]
      interval: 30s
      retries: 20


  tts_service:
    image: ghcr.io/coqui-ai/tts
    container_name: tts_service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "5002:5002"
    entrypoint: /bin/bash
    stdin_open: true
    tty: true
    command: >
      -c "apt update && apt install curl -y && python3 TTS/server/server.py --model_name tts_models/en/vctk/vits"
    healthcheck:
      test: ["CMD-SHELL", "curl -f 'http://localhost:5002/'  || exit 1"]
      interval: 30s
      retries: 20

# IF USING A GPU (CUDA). You need to install the Nvidia container toolkit before running this. Docs: (https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image#:~:text=ollama%20ollama/ollama-,Nvidia%20GPU,-Install%20the%20Nvidia)
  ollama:
    image: ollama/ollama
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ollama:/root/.ollama
      - ./init-scripts/run-ollama.sh:/tmp/run_ollama.sh
    ports:
      - "11435:11434"
    entrypoint: ["/bin/sh", "/tmp/run_ollama.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f 'http://localhost:11434/' || exit 1"]
      interval: 30s
      retries: 20


  postgres:
    image: postgres:latest
    container_name: postgres-container
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=my-secret-pw
      - POSTGRES_DB=postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      retries: 3

volumes:
  postgres_data:
  ollama: